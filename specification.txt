# **AI Triage Assistant: Electron App Specification**

## **Purpose**

A desktop application to triage and classify documents, URLs, and multimedia using local or remote LLMs. All processing and data storage must be local to the user’s machine; no cloud backends.

---

## **Key Features**

### 1. **Artifact Ingestion**

* Drag-and-drop zone and/or file picker for:

  * Files: PDF, DOCX, TXT, images
  * URLs (typed or pasted)
  * (Optional) Pasted or typed free text

### 2. **Content Extraction Modules**

* **PDF:** Extract selectable text (fallback to OCR for image PDFs if possible)
* **Word DOCX:** Extract main text
* **Webpages:** Fetch and extract main content (static by default; optionally JS-rendered with headless browser)
* **YouTube:** Extract transcript/captions when available
* **(Extensible):** Module/plugin architecture for additional artifact types

### 3. **AI Analysis**

* **System prompt** editable via the UI
* **Multiple provider support**:

  * OpenAI (via API key)
  * Anthropic (via API key)
  * Local LLMs (Ollama, llama.cpp, LM Studio) via HTTP API or CLI
* **Pluggable provider/adapters**—user can configure and enable/disable in UI

### 4. **Inbox & History**

* Central sortable/filterable list of all processed artifacts, with:

  * Artifact type/icon (file, link, etc.)
  * Short description (filename or URL)
  * Date/time added
  * **AI recommendation** (e.g., “Read”, “Discard”)—always displayed as a colored label/badge
  * AI’s reasoning (short text)
  * Provider/model used
  * Actions: view details, re-process, delete, copy result

### 5. **Details/Preview Pane**

* When an item is clicked, display:

  * Extracted content (truncated if large)
  * Full AI output (decision & reasoning)
  * Link to original file or web resource

### 6. **Configuration**

* **Provider management UI**:

  * Add/edit/delete API keys or endpoints for LLM providers
  * Set active/default provider/model
  * For local providers, specify API endpoint or executable path
* **System prompt editing**

  * Text area for default system prompt, with save/apply
  * Optionally, allow per-item override
* **User preferences:** (theme, history retention, token limits, etc.)

### 7. **Security/Privacy**

* **All data and configuration must be stored locally** (e.g., SQLite, local filesystem)
* No background cloud synchronization, telemetry, or analytics by default
* (Optional) Export/import for data or settings

### 8. **Extensibility**

* Extraction and provider logic must be **modular/pluggable**
* Simple interface for adding new artifact handlers or LLM adapters
* (Optional) Plugin directory and dynamic loading

---

## **Technical Requirements**

### **Platform**

* ElectronJS (desktop, cross-platform: Windows, macOS, Linux)
* Backend: NodeJS (for file I/O, running subprocesses, API calls)
* Frontend: React, Vue, or Svelte preferred

### **Dependencies**

* Use `pdfplumber` or `PyPDF2` (via Node child process, or call Python scripts) for PDF extraction
* Use suitable npm libraries or subprocess for DOCX/text extraction
* Use `axios`/`node-fetch` for web requests, headless Chromium (Puppeteer/Playwright) for JS-rendered sites
* For local LLMs, use HTTP API where possible (Ollama, llama.cpp server), or shell out to CLI

### **Packaging**

* All app logic, models, and data reside on user’s computer
* Auto-update optional, but must be opt-in and user-controlled

---

## **Out-of-Scope**

* No cloud-hosted backend or persistent cloud storage
* No automatic emailing or messaging unless explicitly requested
* No in-app LLM training or fine-tuning (use external tools for this)

---

## **User Stories (MVP Examples)**

* **As a user,** I can drag in a PDF or paste a link, and see within seconds whether the AI thinks I should “read” or “discard” it, and why.
* **As a user,** I can configure which AI model/provider is used and update my API keys or local endpoints in the UI.
* **As a user,** I can edit the instructions (“system prompt”) that guide the AI’s decision-making.
* **As a user,** I can browse, search, and sort previous artifacts and their recommendations in an inbox/history view.
* **As a user,** I can click an entry to review the extracted content and the AI’s reasoning.

---

## **Future Expansion (Out-of-MVP)**

* Per-item prompt override
* Multi-user support
* Plugin marketplace
* Custom workflows (e.g., auto-forward to other apps)
* Cloud sync/export

---

## **Optional Bonus**

* CLI companion utility for batch automation (e.g., process a folder of PDFs and output a CSV of decisions)
* Save annotated artifacts (decision and reason) to disk alongside originals

---

## **Acceptance Criteria**

* All artifact content and results are processed and stored locally only
* AI system prompt and provider are user-editable in the UI
* Modular support for new file types and models without major rewrite
* Works offline except when making explicit LLM API calls to remote models

---